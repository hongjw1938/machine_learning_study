##  8장. 회귀 : 수치형 값 예측하기
>###    8.1 회귀로 최적선 찾기
        * 선형 회귀
            - 장점 : 결과를 해석하기 쉽고 계산 비용이 적음
            - 단점 : 비선형 데이터를 모델링하기에 부적합
            - 활용 : 수치형, 명목형
        - 회귀방정식은 회귀 가중치를 갖고, 아를 찾는 과정을 회귀라고 한다.
        - 여기서는 선형회귀만 다루도록 한다.
        * 회귀의 일반적 접근법
            - 수집 : 모든 방법
            - 준비 : 회귀를 위한 수치형 자료 주닙. 명목형은 이진형 값으로 변경
            - 분석 : 2D 플롯은 시각화에 도움을 줌. 회귀 가중치도 시각화 가능
            - 훈련 : 회귀 가중치를 찾음
            - 검사 : 모델의 성과를 측정하기 위해 R2 또는 예측 값과 데이터 간의 상관관계 측정할 수 있따.
            - 사용 : 회귀를 가지고 입력 수치에 대한 수치형 값 예측 가능.
        * 회귀 방정식 구하기
            - 입력 데이터는 행렬 X에 담김
            - 벡터 w에는 회귀 가중치가 담김
            - 데이터의 일부인 X_1이 주어진 경우, 예측 값은 y_1 = X_1^T * w로 구함.
            - w를 찾는 방법은 오류를 최소화 하는 것.
            - 제곱 오류는 다음과 같이 표현한다
                - from i=1 to m (y_i - x_i^T * w)^2
            - 이는 OLS(ordinary least squares)이다.
        * loadDataSet함수 : 값을 탭으로 구분한 텍스트 파일을 불러들이고, 마지막에 있는 값을 목적 값으로 가정한다.
        * standRegres함수 : 최적선을 계산하는 함수
            - linalg는 선형대수학 라이브러리이며 linalg.det()을 호출해 xTx가 0인지를 확인핟.
            - 만약 0이라면 역행렬을 구할 수 없다.
        - yHat이 얼마나 잘 예측된 것인지 알고 싶은 경우 실제 데이터 y와 대조하여 두 개의 연속적 데이터 간의 상관관계를 계산하면 된다.
        * np.corrcoef(yEstimate, yActual) : 상관관계를 확인할 수 있다.
>###    8.2 지역적 가중치가 부여된 선형 회귀
        - 선형 회귀의 문제점은 부족적합 경향이 있다는 것
        * 부족적합 : 과소적합이라고도 한다. 이는 데이터를 예측하는 데 중요한 매개변수가 전체 데이터의 크기에 비해 차지하는 양이 적어 모델에서 무시되거나 제외되는 현상을 의미한다.
        - 따라서 추정 값에 성향(bias)를 추가해 평균 제곱 오류를 줄일 수 있는 방법을 다룸.
        * LWLR : locally weighted linear regression
            - 관심이 있는 데이터 점 근처에 있는 다른 데이터 점들에 가중치 부여
            - 즉, 8.1절에서 설명한 것과 같이 유사한 최소 제곱 회귀를 계산.