##  4. 신경망 학습
--------------------------------------
    - 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득
    - 신경망이 학습할 수 있도록 해주는 지표인 손실 함수 소개
>###    4.1 데이터에서 학습
        - 기계학습은 데이터가 매우 중요함.
        - 데이터에서 패턴을 찾고 데이터로 이야기를 만든다.
        - 기계학습은 사람의 개입을 최소화하고 수집한 데이터로부터 패턴을 찾기 위해 시도한다.
        - 이전 손글씨 예제와 같이, 이미지에서 feature를 추출하고 그 패턴을 학습함.
        - 이 규칙을 찾아내는 역할을 기계가 담당함.
        - 신경망(딥러닝)은 그러나, 이미지를 "있는 그대로" 학습한다., 이미지에 포함된 중요 특징까지 '기계'가 스스로 학습 >> 따라서 딥러닝을 종단간 기계학습 이라고도 한다.
        * 훈련 데이터와 시험 데이터
            - 기계학습은 데이터를 훈련 데이터와 시험 데이터로 나눠 학습 및 실험을 수행한다.
            - 나누는 이유는 범용적으로 사용가능한 모델이기 때문이다.
            * 범용 능력
                - 문제를 올바르게 풀어내는 능력
            * 오버피팅
                - 한 데이터셋에만 지나치게 최적화된 상태
>###    4.2 손실 함수
        - 신경망 학습에서는 현재의 상태를 '하나의 지표'로 표현함
        - 그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것.
        - 신경망 학습에서 사용하는 지표는 손실 함수 라고 한다. - 일반적으로 평균 제곱 오차와 교차 엔트로피 오차를 사용함.
        - 손실함수는 신경망 성능의 '나쁨'을 나타내는 지표, 따라서 나쁨을 최소화하는 것은 좋음을 최대화 하는 것.
        * 평균 제곱 오차 : neural_network_04.py참조
            - 가장 많이 쓰이는 손실 함수
            - E = 1/2 * (from k=1 to n)(y_k - t_k)^2
            - y_k : 신경망의 출력, t_k : 정답 레이블, k : 데이터의 차원 수
            - 코드에서 평균 제곱 오차를 기준으로 첫 예의 손실함수가 출력이 작음. 정답에 더 가깝다고 판단.
        * 교차 엔트로피 오차(CEE)
            - 또 다른 손실 함수로서 사용되는 cross entropy error함수이다. 수식은 다음과 같다.
            - E = - (sigma)k * t_k * logy_k(자연로그임)
            - t_k는 정답 레이블로 정답에 해당하는 인덱스의 원소만 1이고 나머지는 0이다.(원-핫 인코딩)
            - y_k는 신경망의 출력이다. 따라서 정답 레이블이 '2'이고 신경망 출력이 0.6이라면 교차 엔트로피 오차는  -log0.6 = 0.51이 된다.
            - 즉, 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정한다. 왜냐하면 t_k에서 정답이 아니면 0을 곱하니까
            - E의 값은 자연로그 그래프를 띄게 된다.
            - 정답일 때의 출력이 1이면 E는 0이 된다. 반대로 출력이 작아질수록 오차 값이 커진다.
            - 코드는 neural_network_04.py의 교차 엔트로피 주석 부분 참고
            - 정답 레이블에 맞는 신경망 출력이 값이 크다면 값이 더 작아지는 것을 확인할 수 있다.
            - delta에 매우 작은 값을 더해주는 이유는, 출력값이 0이면 -inf가 되어 계산이 불가하기 때문ㅇ
        * 미니 배치 학습
            - 훈련 데이터 모두에 대한 손실함수의 합을 구하는 방법
                >> E = -1/N * (sigma n) * (sigma k) * t_nk * logy_nk
                >> 데이터가 N개라면 t_nk는 n번째 데이터의 k번째 값을 의미한다.
                >> y_nk는 신경망의 출력, t_nk는 정답 레이블이다.
                >> N개의 데이터로 손실함수를 확장한 것이다. 다만, N으로 나누어 정규화 함.
                >> 이를 통해, 평균 손실 함수를 구하는 것이다. 언제든 통일된 지표를 얻을 수 있다.
            - 훈련 데이터가 매우 많은 경우
                >> 빅데이터 수준이 되면 많은 데이터를 대상으로 일일이 손실 함수를 계산하는 것은 비현실적
                >> 데이터의 일부를 추려 전체의 '근사치'로 이용 가능
                >> 이 일부를 '미니배치'라고 부른다.(무작위 추출)
                >> 이를 통한 학습을 미니 배치 학습이라고 한다.
            - 교차 엔트로피를 미니 배치 방식으로 변경한 코드 : neural_network_04.py참조
                >> 해당 코드에서 np.log(y[np.arange(batch_size), t])는 fancy 색인이다.
        * 손실 함수를 설정하는 이유
            - 신경망 학습에서는 최적의 매개변수(가중치, 편향)을 탐색할 때, 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾는다.
            - 이 때, 매개변수의 미분을 계산 후, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복함.
            - 가중치 매개변수에 대한 손실 함수의 미분
                >> 가중치 매개 변수의 값을 아주 조금 변화시킬 때, 손실 함수가 어떻게 변하는가
                >> 손실 함수 값 줄이기
                    - 미분 값이 음수 : 해당 가중치 매개변수를 양의 방향으로 변화
                    - 미분 값이 양수 : 해당 가중치 매개변수를 음의 방향으로 변화
                    - 미분 값이 0 : 해당 가중치 매개변수 갱신 중단
            - 신경망 학습 시 정확도를 지표로 삼아선 안됨.
                >> 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없다.
                    - 왜 대부분의 장소에서 매개변수의 미분이 0이 되는가?
                        >> 가중치를 변화시켜도 정확도는 그대로이기 때문이다. 
                        >> 혹시라도 정확도가 개선되더라도 연속적인 변화보다는 불연속적인 값으로 바뀌기 때문이다.
                        >> 계단 함수를 활성화 함수로 사용하지 않는 이유도 이와 같다
            - 손실함수를 지표로 사는 경우
                >> 매개변수의 값이 변하면 그에 반응해 손실 함수의 값도 연속적으로 변화한다.
            - 그래서, 계단 함수는 한순간만 변화가 있고 대부분의 장소에서 기울기가 0이지만, 시그모이드는 값이 연속적으로 변하며 곡선의 기울기도 연속적으로 변하기 때문에 신경망이 올바르게 학습할 수 있다.
>###    4.3 수치 미분
        * 미분
            - 미분은 '순간 변화량'을 의미한다.
            - 미분을 python으로 구현시에 너무 작은 값은 반올림 오차 문제를 일으키므로 10^-4정도의 값으로 개선한다.
            * 중심차분 or 중앙차분
                - (x + h)와 (x - h)의 함수 f의 차분을 계산하는 것.
                - (x + h)와 x 사이의 차분은 전방 차분이다.
                - 이 수치 미분은 해석적 미분(실제 공식에서 사용하는 미분)을 근사치로 계산하는 방식이다.
                - 따라서 정확한 미분값이 도출되지는 않지만 거의 비슷하다.
        * 편미분
            - 변수가 여럿인 함수에 대해 특정 변수에 대해 미분하는 경우
            - 여러 변수 중 목표 변수 하나에 초점을 맞추고 다른 변수는 값을 고정한다.
>###    4.4 기울기
        * 기울기
            - 모든 변수의 편미분을 벡터로 정리한 것.
            - gradient함수는 이전의 함수와 같이 미분, 즉 기울기를 구하는 공식을 구현한 것이다.
            - 기울기가 의미하는 것은, 방향을 가진 벡터이다. 즉, 기울기의 그림을 그리면 기울기가 함수의 가장 낮은 장소를 가리키는 것과 같다.
            - 즉, 출력값을 가장 크게 줄이는 방향
            - neural_network_04.py에서 기울기 부분의 코드는 각 변수의 기울기를 편미분 하며 정리한 반환한다.
        * 경사법(경사 하강법)
            - 신경망은 최적의 매개변수를 학습 시에 찾아야 한다. 최적이란, 손실 함수가 최솟값이 될 때의 매개변수 값을 의미한다.
            - 일반적으로 손실 함수는 매우 복잡하다. 공간이 광대해 어디가 최솟값인지 알 수 없다. 이 상황에서 기울기를 잘 사용해 함수의 최솟값을 찾으려는 것이 경사법
            - 함수의 값을 낮추는 방안을 제시하는 지표가 기울기임!(하지만 기울기가 가리키는 방향에 최솟값이 없는 경우가 많다.)
            - 실제로 함수가 극솟값, 최솟값, 안장점이 되는 장소는 기울기가 0이다. 그러나, 0이라고 해서 반드시 최소값이라고는 할 수 없다. 함수가 복잡하면 국소적으로 극소값인 경우가 있을 수 있으며, 고원이라고 하는 학습이 진행되지 않는 정체기에 빠질 수도 있다.
            * 개념
                - 경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동하고, 이동한 위치에서 기울기를 구하고 또 이동하기를 반복한다. 이 방식으로 함수의 값을 줄이는 것이 경사법이며, 기계학습을 최적화하는 데 사용된다.
                - 최대값을 찾는 경우 경사 상승법, 최소값인 경우 경사 하강법
            - 수식으로 나타내면 다음과 같다.
            <img src="/python_class/deep_learning/deep learning from scratch/ch04_nn_train_image/gradient_descent.JPG">
                - 여기서 기호 eta(에타)는 갱신하는 양을 나타낸다. 이를 신경망 학습에서는 학습률이라고 한다.
            - 코드(neural_network_04.py 경사법 부분 참조)
                - lr는 학습률이다. lr이 너무 크거나 작으면 좋은 결과를 얻을 수 없다.
                - 결과는 (0, 0)에 가까운 결과를 얻을 수 있다.
            * 학습률
                - 학습률과 같은 매개변수를 하이퍼파라미터라고 한다.
                - 사람이 직접 설정해야 하며 여러 후보 값 중에서 시험을 통해 가장 잘 학습하는 값을 찾는 과정을 거쳐야만 한다.
                - 학습률이 너무 크면 큰 값으로 발산해버리며, 너무 작으면 갱신이 충분히 되지 않는다.
        * 신경망에서의 기울기
            - 신경망 학습에서도 기울기를 구해야 한다. 이 기울기는 가중치 매개변수에 대한 손실 함수의 기울기이다.
            - 가중치가 W, 손실 함수가 L인 신경망에서 경사는 ∂L/∂W 로 나타난다. 즉, 가중치에 대한 손실함수 출력값의 순간 변화량
            - simpleNet부분 코드를 확인
            - loss는 손실함수를 의미
>###    4.5 학습 알고리즘 구현하기
        * 학습 절차
            - 전체 : 신경망에는 적응 가능한 가중치, 편향이 있고 이것을 훈련 데이터에 적응하도록 조정하는 과정이 '학습'이다.
            - 1단계 - 미니배치 : 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표
            - 2단계 - 기울기 산출 : 각 가중치 매개변수의 기울기를 구함. 기울기는 손실 함수의 값을 가장 작게 하는 방향 제시
            - 3단계 - 매개변수 갱신 : 가중치 매개변수를 기울기 방향으로 조금 갱신
            - 4단계 - 반복
        * 확률적 경사 하강법(SGD) : 데이터를 미니배치로 무작위로 선정함.
        * 2층 신경망 클래스 구현하기
            - TwoLayerNet으로 클래스 구현(코드 참조)
            * 에폭 : 하나의 단위, 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당함
            
        