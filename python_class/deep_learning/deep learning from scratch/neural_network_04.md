##  4. 신경망 학습
--------------------------------------
    - 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득
    - 신경망이 학습할 수 있도록 해주는 지표인 손실 함수 소개
>###    4.1 데이터에서 학습
        - 기계학습은 데이터가 매우 중요함.
        - 데이터에서 패턴을 찾고 데이터로 이야기를 만든다.
        - 기계학습은 사람의 개입을 최소화하고 수집한 데이터로부터 패턴을 찾기 위해 시도한다.
        - 이전 손글씨 예제와 같이, 이미지에서 feature를 추출하고 그 패턴을 학습함.
        - 이 규칙을 찾아내는 역할을 기계가 담당함.
        - 신경망(딥러닝)은 그러나, 이미지를 "있는 그대로" 학습한다., 이미지에 포함된 중요 특징까지 '기계'가 스스로 학습 >> 따라서 딥러닝을 종단간 기계학습 이라고도 한다.
        * 훈련 데이터와 시험 데이터
            - 기계학습은 데이터를 훈련 데이터와 시험 데이터로 나눠 학습 및 실험을 수행한다.
            - 나누는 이유는 범용적으로 사용가능한 모델이기 때문이다.
            * 범용 능력
                - 문제를 올바르게 풀어내는 능력
            * 오버피팅
                - 한 데이터셋에만 지나치게 최적화된 상태
>###    4.2 손실 함수
        - 신경망 학습에서는 현재의 상태를 '하나의 지표'로 표현함
        - 그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것.
        - 신경망 학습에서 사용하는 지표는 손실 함수 라고 한다. - 일반적으로 평균 제곱 오차와 교차 엔트로피 오차를 사용함.
        - 손실함수는 신경망 성능의 '나쁨'을 나타내는 지표, 따라서 나쁨을 최소화하는 것은 좋음을 최대화 하는 것.
        * 평균 제곱 오차 : neural_network_04.py참조
            - 가장 많이 쓰이는 손실 함수
            - E = 1/2 * (from k=1 to n)(y_k - t_k)^2
            - y_k : 신경망의 출력, t_k : 정답 레이블, k : 데이터의 차원 수
            - 코드에서 평균 제곱 오차를 기준으로 첫 예의 손실함수가 출력이 작음. 정답에 더 가깝다고 판단.
        * 교차 엔트로피 오차(CEE)
            - 또 다른 손실 함수로서 사용되는 cross entropy error함수이다. 수식은 다음과 같다.
            - E = - (sigma)k * t_k * logy_k(자연로그임)
            - t_k는 정답 레이블로 정답에 해당하는 인덱스의 원소만 1이고 나머지는 0이다.(원-핫 인코딩)
            - y_k는 신경망의 출력이다. 따라서 정답 레이블이 '2'이고 신경망 출력이 0.6이라면 교차 엔트로피 오차는  -log0.6 = 0.51이 된다.
            - 즉, 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정한다. 왜냐하면 t_k에서 정답이 아니면 0을 곱하니까
            - E의 값은 자연로그 그래프를 띄게 된다.
            - 정답일 때의 출력이 1이면 E는 0이 된다. 반대로 출력이 작아질수록 오차 값이 커진다.
            - 코드는 neural_network_04.py의 교차 엔트로피 주석 부분 참고
            - 정답 레이블에 맞는 신경망 출력이 값이 크다면 값이 더 작아지는 것을 확인할 수 있다.
            - delta에 매우 작은 값을 더해주는 이유는, 출력값이 0이면 -inf가 되어 계산이 불가하기 때문ㅇ
        * 미니 배치 학습
            - 훈련 데이터 모두에 대한 손실함수의 합을 구하는 방법
                >> E = -1/N * (sigma n) * (sigma k) * t_nk * logy_nk
                >> 데이터가 N개라면 t_nk는 n번째 데이터의 k번째 값을 의미한다.
                >> y_nk는 신경망의 출력, t_nk는 정답 레이블이다.
                >> N개의 데이터로 손실함수를 확장한 것이다. 다만, N으로 나누어 정규화 함.
                >> 이를 통해, 평균 손실 함수를 구하는 것이다. 언제든 통일된 지표를 얻을 수 있다.
            - 훈련 데이터가 매우 많은 경우
                >> 빅데이터 수준이 되면 많은 데이터를 대상으로 일일이 손실 함수를 계산하는 것은 비현실적
                >> 데이터의 일부를 추려 전체의 '근사치'로 이용 가능
                >> 이 일부를 '미니배치'라고 부른다.(무작위 추출)
                >> 이를 통한 학습을 미니 배치 학습이라고 한다.
            - 교차 엔트로피를 미니 배치 방식으로 변경한 코드 : neural_network_04.py참조
                >> 해당 코드에서 np.log(y[np.arange(batch_size), t])는 fancy 색인이다.
        * 손실 함수를 설정하는 이유
            - 신경망 학습에서는 최적의 매개변수(가중치, 편향)을 탐색할 때, 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾는다.
            - 이 때, 매개변수의 미분을 계산 후, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복함.
            - 가중치 매개변수에 대한 손실 함수의 미분
                >> 가중치 매개 변수의 값을 아주 조금 변화시킬 때, 손실 함수가 어떻게 변하는가
                >> 손실 함수 값 줄이기
                    - 미분 값이 음수 : 해당 가중치 매개변수를 양의 방향으로 변화
                    - 미분 값이 양수 : 해당 가중치 매개변수를 음의 방향으로 변화
                    - 미분 값이 0 : 해당 가중치 매개변수 갱신 중단
            - 신경망 학습 시 정확도를 지표로 삼아선 안됨.
                >> 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없다.
                    - 왜 대부분의 장소에서 매개변수의 미분이 0이 되는가?
                        >> 가중치를 변화시켜도 정확도는 그대로이기 때문이다. 
                        >> 혹시라도 정확도가 개선되더라도 연속적인 변화보다는 불연속적인 값으로 바뀌기 때문이다.
                        >> 계단 함수를 활성화 함수로 사용하지 않는 이유도 이와 같다
            - 손실함수를 지표로 사는 경우
                >> 매개변수의 값이 변하면 그에 반응해 손실 함수의 값도 연속적으로 변화한다.
            - 그래서, 계단 함수는 한순간만 변화가 있고 대부분의 장소에서 기울기가 0이지만, 시그모이드는 값이 연속적으로 변하며 곡선의 기울기도 연속적으로 변하기 때문에 신경망이 올바르게 학습할 수 있다.
            