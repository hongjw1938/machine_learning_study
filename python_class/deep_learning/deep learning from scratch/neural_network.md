##  3. 신경망
>###    3.1 퍼셉트론에서 신경망으로
        * 신경망의 장점
            - 퍼셉트론의 경우 복잡한 함수를 표현할 수 있으나, 가중치를 수동으로 해결해야 한다는 문제점
            - 신경망은 이 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력이 있음.
        * 신경망의 예
            <img src="/python_class/deep_learning/deep learning from scratch/ch03_nn_image/nn.JPG">
            - 위의 사진 처럼 기장 왼쪽부터 입력층, 은닉층, 출력층으로 부른다.
            - 은닉층은 사람 눈에 보이지 않음.
            - 0층 : 입력층, 1층 : 은닉층, 2층 : 출력층
            - 가중치를 갖는 층은 2개뿐이라서 '2층 신경망'으로 부른다.
            - 실제 뉴런이 연결되는 방식은 퍼셉트론과 같음.
        * 활성화함수
            - 퍼셉트론에서 h(x) = { 0 if (x <= 0), 1 if (x > 0)}와 같이 입력 신호의 총합을 출력신호로 변환하는 함수가 등장했음.
            - 이 함수가 활성화 함수임. 즉, 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할
            * 활성화함수의 처리과정
                <img src="/python_class/deep_learning/deep learning from scratch/ch03_nn_image/activation_func.JPG">
                - 그림과 같이 기존 뉴런의 원을 키우고 활성화 함수의 처리 과정을 명시적으로 그려넣음.
                - 즉, 가중치 신호를 조합한 결과가 a라는 노드가 되며, 활성화 함수 h()를 통과해 y라는 노드로 변환됨.
                - 뉴런과 노드는 같은 의미.
>###    3.2 활성화함수
        * 계단함수
            - h(x) = 0 if (x <= 0), 1 if (x > 0)
            - 임계값을 경계로 출력값이 바뀐다.
            - 퍼셉트론은 계단 함수를 활성화함수로 사용한다.
        * 시그모이드 함수
            - h(x) = 1 / (1 + exp(-x))
            - 신경망에서는 활성화 함수로 시그모이드 함수를 이용해 신호를 변환한다.
            - 해당 신호를 다음 뉴런에 전달함.
        * 계단함수, 시그모이드함수 구현 : neural_network.py참조
        * 비선형 함수
            - 계단 함수와 시그모이드의 공통점은 둘 다 비선형이라는 것.
            - 신경망은 비선형을 사용해야 한다.
            * 선형이 안되는 이유?
                - 선형 함수는 층을 아무리 깊게 하더라도 은닉층이 없는 네트워크 로도 같은 기능이 가능하다
                - h(x) = cx를 활성화함수로 쓰는 3층 네트워크의 경우
                - y(x) = h(h(h(x)))이므로 y = c^3x = ax와 같다.
                - 즉, 은닉층의 이점을 살릴 수가 없다.
        * ReLU함수(구현은 neural_network.py 참조)
            - 신경망 분야에서 요즘에는 이 함수를 많이 차용함.
            - 0 초과이면 입력 자체를 출력하고 0 이하이면 0을 출력
            - h(x) = { x if (x > 0), 0 if (x <= 0)
>###    3.3 다차원 배열의 계산
        * 행렬
            - 행렬의 곱 : np.dot()함수로 계산 (1차원이면 벡터를, 2차원이면 행렬 곱 계산)
            - 마땅히 대응하는 차원의 원소의 갯수를 맞추어야 한다.
>###    3.4 3층 신경망 구현
        - 3층 신경망 : 입력층(0층) 2개, 첫 은닉층(1층) 3개, 두 번째 은닉층(2층) 2개, 출력층(3층) 2개의 뉴런으로 구성
        * 표기법
            <img src="/python_class/deep_learning/deep learning from scratch/ch03_nn_image/notation.JPG">
            - w^(1)_12 나 a^(1)_1 같은 표기법이 나옴.
            - 위첨자 (1)과 같은 경우 1층의 가중치라는 의미(즉, 1층의 뉴런임을 의미)
            - 아래첨자 12에서 1은 다음 층의 1번째 뉴런, 2는 앞 층의 두 번째 뉴런을 의미
            - 즉, w^(1)_12는 앞 층의 2번째 뉴런(x_2)에서 다음 층의 1번째 뉴런(a^(1)_1)로 향할 때의 가중치라는 의미이다.
        * 각 층의 신호 전달 구현
            <img src="/python_class/deep_learning/deep learning from scratch/ch03_nn_image/signaling.JPG">
            - 위 그림에서 1은 편향을 의미하는 뉴런임. 편향은 아래첨자 인덱스가 하나 뿐. 왜냐하면 앞 층의 편향 뉴런이 하나뿐이기 때문이다.
            - a^(1)_1을 수식으로 나타내면
                --> a^(1)_1 = w^(1)_11 * x_1 + w^(1)_12 * x_2 + b^(1)_1
                --> A^(1) = X * W^(1) + B^(1) : 행렬식으로 나타낸 것.
            * 각 행렬
                - A^(1) : (a^(1)_1, a^(1)_2, a^(1)_3)
                - X = (x_1, x_2)
                - B^(1) = (b^(1)_1, b^(1)_2, b^(1)_3)
                - W^(1) = (w^(1)_11, w^(1)_21, w^(1)_31,
                           w^(1)_12, w^(1)_22, w^(1)_32)
            - 구현 : neural_network.py 의 주석 signaling부분 참조
        * 1층 활성화 함수 처리(sigmoid)
            <img src = "/python_class/deep_learning/deep learning from scratch/ch03_nn_image/step1_sigmoid.JPG">
            - h()가 여기서는 sigmoid함수를 의미한다.
            - 은닉층에서는 가중치의 합을 a로 표기하고 활성화 함수 h()로 변환된 신호를 z로 표기한다.
            - 구현은 neural_network.py참조
        * 2층 신호 전달 
            <img src="/python_class/deep_learning/deep learning from scratch/ch03_nn_image/signaling_2nd_step.JPG">
        * 마지막 구현 : neural_network.py 참조
            - 출력층의 활성화 함수는 풀고자 하는 문제의 성질에 맞게 결정함.
            - 회귀에는 항등 함수, 2클래스 분류는 시그모이드, 다중 클래스 분류는 소프트맥스 함수를 사용하는 것이 일반적
>###    3.5 출력층 설계하기
        ** 기계학습은 일반적으로 분류와 회귀로 나뉨. 분류는 데이터가 어느 클래스에 속하느냐는 문제이고, 회귀는 입력 데이터에서 연속적 수치를 예측하는 문제임.
            --> 일반적으로 회귀에는 항등, 분류에는 소프트맥스 함수를 사용
        * 항등함수와 소프트맥스 함수 구현 : neural_network.py의 softmax 부분 참조
            >> 항등함수 : 입력 신호가 그대로 출력신호가 됨.
            >> 소프트맥스 함수 : y_k = exp(a_k) / (from i=1 to n)exp(a_i)
                - n : 출력층의 뉴런 수, y_k : k번째 출력임을 의미
        * 소프트함수 구현시 주의점
            - 컴퓨터 계산시 오버플로우 문제가 발생할 수 있다. (e^1000이면 inf가 된다.)
            - 지수함수의 문제에 의해 결과 수치가 불안정해진다.
            - 따라서 이 문제를 해결하기 위해 C라는 임의의 정수를 분모/분자에 곱해 해결함.
            - 출력 총합이 1이 되는 것은 소프트맥스 함수의 중요한 성질 -> 이에 의해 함수 출력 값을 '확률'로 해석 가능
            - 즉, 소프트맥스 함수를 이용해 문제를 확률적으로 대응할 수 있다.
            - 주의점 : 소프트맥스 함수를 적용하더라도 각 원소의 대소 관계는 불변. exp(x) = y가 단조 증가함수이기 때문
            - 그래서, 출력이 가장 큰 뉴런의 위치는 불변이기 때문에 출력층에서 굳이 소프트맥스 함수를 명시하지 않는 경우도 있다.
        * 출력층의 뉴런 수 정하기
            - 출력층의 뉴런 수는 풀려는 문제에 맞게 적절히 지정해야 한다.
            - 분류에서는 분류하고 싶은 클래스 수로 설정하는 것이 일반적
            - 예를 들어, 이미지를 숫자 0부터 9 중 하나로 분류한다면 출력층 뉴런은 10개
>###    3.6 손글씨 숫자 인식
        - 학습된 매개변수를 사용해 학습 과정 생략하고, 추론 과정을 구현
        - 이 추론 과정을 신경망의 순전파(forward propagation)라고도 함.
        1. MNIST 데이터 셋
            - 기계학습 분야의 아주 유명한 데이터셋
            - 손글씨 숫자 이미지 집합
            - 0~9의 숫자 이미지로 구성되며 훈련 이미지 6만장, 시험 이미지 1만장이 존재
            - http://yann.lecun.com/exdb/mnist/ 참조
            * load_mnist함수
                - 읽은 MNIST데이터를 "(훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블)" 형식으로 반환
                - 인수 : normalize, flatten, one_hot_label을 설정 가능 세 개 모두 bool 값
                - normalize : 입력 이미지의 픽셀 값을 0.0 ~ 1.0 사이로 정규화할지 결정, False이면 0~255 사이의 값을 유지
                - flatten : 입력이미지를 평탄하게, 1차원 배열로 만들지 결정, False이면 1 * 28 * 28의 3차원 배열로, True면 784개의 원소로 이뤄진 1차원 배열로 저장 
                - one_hot_label : 레이블을 one_hot_encoding형태로 저장할지 결정, 원-핫 인코딩이란, 정답인 원소만 1이고 나머지는 0인 배열을 의미
            * PIL(Python Image Library)
                - pillow 패키지 다운
                - 이미지 표시에 사용하는 모듈
                - numpy로 저장된 이미지 데이터를 PIL용 데이터 객체로 변환시 Image.fromarray()가 수행함.
        2. 신경망의 추론 처리
            - MNIST데이터셋의 입력층 뉴런은 784개, 출력층은 10개임. 784개인 이유는 이미지 크기가 28 * 28 이기 때문
            - 첫 은닉, 두번째 은닉은 임의의 값인 50, 100으로 결정
            * get_data()함수
                - MNIST dataset을 가져옴
            * init_network()함수
                - sample_weight.pkl에 저장된 '학습된 가중치 매개변수'를 읽음
                - 해당 파일에는 가중치와 편향 매개변수가 딕셔너리 변수로 저장되어 있음
            * predict()함수
                - 신경망으로 예측
            * 순서
                - 우선 MNIST 데이터셋을 얻고 네트워크를 생성
                - for반복을 순환며 x에 저장된 이미지 데이터를 predict로 분류
                - predict함수는 각 레이블의 학률을 넘파이 배열로 반환 ex) [0.1, 0.3, 0.2, ... , 0.04]
                - 이는 이미지가 숫자 '0'일 확률이 0.1, '1'일 화귤이 0.3 등으로 해석한다.
                - 그 다음, np.argmax() 함수로 이 배열에서 값이 가장 큰 원소의 인덱스를 구함. 그것이 예측 결과임
                - 마지막으로 신경망 예측 값과 실제 레이블을 비교해 정확도를 구함
            * 정규화 : 데이터를 특정 범위로 변환 -- > normalize
            * 전처리 : 입력 데이터에 특정 변환을 가하는 것.
        3. 배치 처리
            * 위의 신경망 추론은 각 층 배열 형상 추이가 아래와 같다.
                - X     W1      W2      W3      Y
                - 784   784*50  50*100  100*10  10
                - 즉, 1장만 입력시에 위와 같은 처리 흐름을 거치게 되는 것이다.
            * 배치 처리
                - 이미지 여러 장을 한꺼번에 입력하는 경우가 있을 수 있다.
                - 이미지 100개를 묶어 predict함수에 한 번에 넘긴다.
                - 아래와 같은 형상이 가능
                - X         W1      W2      W3      Y
                - 100*784   784*50  50*100  100*10  10
                - 이와 같이 하나로 묶은 입력 데이터를 배치라고 부른다.
            - 배치 처리를 통해서 효율적이고 빠르게 데이터를 처리할 수 있다.